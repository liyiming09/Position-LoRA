{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import torch, cv2\n",
    "from PIL import Image\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from templates.templates import inference_templates\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Optional, Union, Tuple, List, Callable, Dict\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "import torch.nn.functional as nnf\n",
    "import numpy as np\n",
    "import abc\n",
    "# import ptp_utils\n",
    "# import seq_aligner\n",
    "import shutil\n",
    "from torch.optim.adam import Adam\n",
    "from PIL import Image\n",
    "from NullextInversion import NullInversion\n",
    "import math\n",
    "from lora import (\n",
    "    save_lora_weight,\n",
    "    TEXT_ENCODER_DEFAULT_TARGET_REPLACE,\n",
    "    get_target_module,\n",
    "    save_lora_layername,\n",
    "    monkeypatch_or_replace_lora,\n",
    "    monkeypatch_remove_lora,\n",
    "    set_lora_requires_grad,\n",
    "    tune_lora_scale\n",
    ")\n",
    "from utils import encoder, p2p, ptp_utils\n",
    "\n",
    "def make_image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    h, w = imgs[0].shape[0],imgs[0].shape[1]\n",
    "    grid = np.zeros((rows*h, cols*w, 3)).astype(np.uint8)\n",
    "    \n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        hi = i // cols  # Corrected: Calculate row index\n",
    "        wi = i % cols   # Corrected: Calculate column index\n",
    "        grid[hi*h:hi*h+h, wi*w:wi*w+w,:] = img.copy()\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOW_RESOURCE = False \n",
    "NUM_DDIM_STEPS = 50\n",
    "GUIDANCE_SCALE = 7.5\n",
    "MAX_NUM_WORDS = 77\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"\")\n",
    "    parser.add_argument(\n",
    "        \"--prompt\",\n",
    "        type=str,\n",
    "        default=\"A man {} a man\",\n",
    "        help=\"input a single text prompt for generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--template_name\",\n",
    "        type=str,\n",
    "        help=\"select a batch of text prompts from templates.py for generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_id\",\n",
    "        type=str,\n",
    "        default = \"/internfs/xxxxxx/huggingface_models/stable-diffusion-v1-5\",\n",
    "        help=\"absolute path to the folder that contains the trained results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--exp_id\",\n",
    "        type=str,\n",
    "        help=\"absolute path to the folder that contains the trained results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--inference_string\",\n",
    "        type=str,\n",
    "        default=\"<R>\",\n",
    "        help=\"inference_string of the relation prompt\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--placeholder_string\",\n",
    "        type=str,\n",
    "        default=\"<R>\",\n",
    "        help=\"place holder string of the relation prompt\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_samples\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of samples to generate for each prompt\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--guidance_scale\",\n",
    "        type=float,\n",
    "        default=7.5,\n",
    "        help=\"scale for classifier-free guidance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--strength\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"strength for inversion\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--only_load_embeds\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If specified, the experiment folder only contains the relation prompt, but does not contain the entire folder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If specified, utilize lora weights for the generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora_scale\",\n",
    "        type=float,\n",
    "        default=0.8,\n",
    "        help=\"scale for lora weights\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--noise\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If specified, utilize noise weights for the generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrain\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--pretrain_id\",\n",
    "        type=str,\n",
    "        default = \"/internfs/xxxxxx/concept_customization/noise_opti/shake_hands\",\n",
    "        help=\"absolute path to the folder that contains the trained results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--ori\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If specified, utilize original noise weights for the generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--splitid\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If specified, utilize noise weights for the generation\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--paste\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"If specified, copy the trained noise into the specific patch(bbox) for the intial noise\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dp\",\n",
    "        action=\"store_true\",\n",
    "        help=\"to do a dp image training or a natural relation training\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_dir\",\n",
    "        type=str,\n",
    "        default='./reversion_benchmark_v1/shake_hands',\n",
    "        # required=True,\n",
    "        help=\n",
    "        \"The folder that contains the exemplar images (and coarse descriptions) of the specific relation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--offset_scale\",\n",
    "        type=float,\n",
    "        default=0.8,\n",
    "        help=\"scale for offset_noise weights\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--encoder\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        # required=True,\n",
    "        help=\n",
    "        \"The folder that contains the encoder of ddim-style noise.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--noise_mode\",\n",
    "        type=str,\n",
    "        default='warp',\n",
    "        help=\"mode to get the initial latent noise, such as random, DDIM, encoder, or else.\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=[\"--only_load_embeds\", \"--lora_scale\",\"0.6\", \"--lora\",\"--noise\"\n",
    "                ])\n",
    "    return args\n",
    "args = parse_args()\n",
    "\n",
    "# args.lora_root = \"noise_astar_diffe/n1.0_a0.0_m0.1_detach/shake_hands_t0.0002_lora0.0001\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.lora_scale=0.6\n",
    "args.exp_id = \"your/path\"     # lora weights\n",
    "\n",
    "# args.exp_id = f\"./noise_astar_diffe/n1.0_a0.0_m{mix_weights[e1]}_detach/shake_hands_t{lrs[e2]}_lora{loras[e3]}\"\n",
    "\n",
    "args.encoder = \"your/path\" # noise encoder\n",
    "\n",
    "args.lora_root = args.exp_id\n",
    "\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False, steps_offset = 1)\n",
    "MY_TOKEN = ''\n",
    "if args.train_data_dir[-1] == '/': args.train_data_dir = args.train_data_dir[:-1] \n",
    "noise_embeds_root = args.train_data_dir + '_noise'\n",
    "local_f = open(os.path.join(args.train_data_dir, 'text.json'))\n",
    "templates = json.load(local_f, object_pairs_hook=OrderedDict)\n",
    "noise_dict = {}\n",
    "for index, (key, value) in enumerate(templates.items()):\n",
    "    noise_dict[key] = index\n",
    "local_f = open(os.path.join(args.train_data_dir, 'bbox.json'))\n",
    "all_bboxes = json.load(local_f, object_pairs_hook=OrderedDict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create inference pipeline\n",
    "if args.only_load_embeds:\n",
    "\n",
    "    print('load relation prompt only')\n",
    "    learned_lora = None\n",
    "    # ddim_noise = None\n",
    "    for filename in os.listdir(args.exp_id):\n",
    "        if filename.endswith('loraemb.pth'):\n",
    "            embed_path = os.path.join(args.lora_root, filename)\n",
    "            learned_embeds = torch.load(embed_path, weights_only=True)\n",
    "        elif filename.endswith('lora.pth'):\n",
    "            lora_path = os.path.join(args.exp_id, filename)\n",
    "            learned_lora = torch.load(lora_path, weights_only=True)\n",
    "        # elif filename.endswith('noisemb.pth'):\n",
    "    exp_name = args.train_data_dir.split('/')[-1]\n",
    "    noise_path = os.path.join(args.pretrain_id, f'{exp_name}_noisemb_ori.pth' if args.ori else f'{exp_name}_noisemb.pth')\n",
    "\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(args.model_id,torch_dtype=torch.float32, scheduler=scheduler).to(\"cuda\")\n",
    "    \n",
    "    text_encoder = pipe.text_encoder\n",
    "    tokenizer = pipe.tokenizer\n",
    "    unet = pipe.unet\n",
    "    \n",
    "    # keep original embeddings as reference\n",
    "    orig_embeds_params = text_encoder.get_input_embeddings().weight.data.clone()\n",
    "\n",
    "    # Add the placeholder token in tokenizer\n",
    "    tokenizer.add_tokens(args.placeholder_string)\n",
    "    text_encoder.get_input_embeddings().weight.data = torch.cat((orig_embeds_params, orig_embeds_params[0:1]))\n",
    "    text_encoder.resize_token_embeddings(len(tokenizer)) \n",
    "\n",
    "    # Let's make sure we don't update any embedding weights besides the newly added token\n",
    "    placeholder_token_id = tokenizer.convert_tokens_to_ids(args.placeholder_string)\n",
    "    index_no_updates = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "    text_encoder.get_input_embeddings().weight.data[index_no_updates] = orig_embeds_params\n",
    "    text_encoder.get_input_embeddings().weight.data[placeholder_token_id] = learned_embeds[0]\n",
    "\n",
    "\n",
    "    if args.lora and (learned_lora is not None) and args.lora_scale>0:\n",
    "        unet_lora_params = None\n",
    "        use_lora_extended = False\n",
    "        lora_unet_rank = 32\n",
    "        lora_txt_rank = 32\n",
    "        injectable_lora = get_target_module(\"injection\", use_lora_extended)\n",
    "        target_module = get_target_module(\"module\", use_lora_extended)\n",
    "\n",
    "        monkeypatch_or_replace_lora(unet, learned_lora, r = lora_unet_rank)\n",
    "\n",
    "        tune_lora_scale(unet, args.lora_scale)\n",
    "else:\n",
    "    # now this works\n",
    "    print('load full model')\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(args.model_id,torch_dtype=torch.float32).to(\"cuda\")\n",
    "\n",
    "unet.eval()\n",
    "unet.enable_gradient_checkpointing()\n",
    "# inversion for a existing image\n",
    "null_inversion = NullInversion(pipe)\n",
    "blend_word = None\n",
    "key_words = ['man']\n",
    "\n",
    "# blend_word = ((('people')))\n",
    "eq_params = None\n",
    "args.strength = 1.0\n",
    "\n",
    "noise_embedding_encoder = encoder.Warpper()\n",
    "noise_embedding_encoder = torch.load(os.path.join(args.encoder,'{}_noisewarpper.pth'.format(exp_name)))\n",
    "            \n",
    "    \n",
    "\n",
    "# single text prompt\n",
    "if args.prompt is not None:\n",
    "    prompt_list = [args.prompt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for copy-paste methods: random sample or user decide a exemplar image, and load its DDIM inversion noise\n",
    "args.splitid = \"7-1\"\n",
    "if f'{args.splitid}.png' in all_bboxes.keys():\n",
    "    splitid_name = f'{args.splitid}.png'\n",
    "else:\n",
    "    splitid_name = f'{args.splitid}.jpg'\n",
    "bboxes = all_bboxes[splitid_name] \n",
    "ddim_noise =  torch.load(os.path.join(noise_embeds_root, f'{args.splitid}.pth')  ,map_location = device, weights_only=True)\n",
    "if args.splitid is not None: \n",
    "    relation_index = noise_dict[splitid_name]\n",
    "src_img = cv2.imread(os.path.join(args.train_data_dir, splitid_name))\n",
    "h, w = src_img.shape[0], src_img.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_list[0]\n",
    "prompt = prompt.lower().replace(\"<r>\", \"<R>\").format(args.inference_string.replace('-',' '))\n",
    "\n",
    "\n",
    "resolution = 512\n",
    "\n",
    "\n",
    "args.encoder = 'noise_new_encoder_shake_hands/'\n",
    "noise_embedding_encoder = torch.load(os.path.join(args.encoder,'{}_noisewarpper.pth'.format(exp_name)))\n",
    "\n",
    "\n",
    "prompt = \"a woman <R> a woman\"\n",
    "# random_seed = 123\n",
    "# torch.cuda.manual_seed_all(random_seed)\n",
    "\n",
    "noise = torch.randn_like(ddim_noise)\n",
    "mix_noise = noise.clone()\n",
    "noise_offsets = []\n",
    "mix_weights = 1\n",
    "R_attn_mask = torch.zeros((resolution//8, resolution//8)).to(\"cuda\")\n",
    "\n",
    "min_height, min_width, max_height, max_width = bboxes[0]\n",
    "mask = torch.zeros((1,1,noise.shape[2], noise.shape[3])).to(device)\n",
    "mask[0:1,:,min_height//8:max_height//8, min_width//8:max_width//8] = 1\n",
    "\n",
    "input_warp = torch.cat([mask, noise], 1)\n",
    "output_warp = noise_embedding_encoder(input_warp)\n",
    "base_encoder_noise = output_warp[:,:,min_height//8:max_height//8, min_width//8:max_width//8].clone()\n",
    "base_mix_noise = ddim_noise[:,:,min_height//8:max_height//8, min_width//8:max_width//8].clone()\n",
    "\n",
    "bbox_fix = []\n",
    "mix_sets = []\n",
    "encoder_sets = []\n",
    "grid_size = 8\n",
    "\n",
    "height_fix, width_fix = -8 * 8, 10 * 8\n",
    "        \n",
    "mix_noise = noise.clone()\n",
    "for bbox in bboxes:\n",
    "    min_height, min_width, max_height, max_width = bbox\n",
    "    min_width += width_fix\n",
    "    max_width += width_fix\n",
    "    min_height += height_fix\n",
    "    max_height += height_fix\n",
    "    mix_noise[:,:,min_height//8:max_height//8, min_width//8:max_width//8] = base_mix_noise \n",
    "\n",
    "    R_attn_mask[min_height//8:max_height//8, min_width//8:max_width//8] = 1\n",
    "\n",
    "\n",
    "    mask = torch.zeros((1,1,noise.shape[2], noise.shape[3])).to(device)\n",
    "    mask[0:1,:,min_height//8:max_height//8, min_width//8:max_width//8] = 1\n",
    "\n",
    "    input_warp = torch.cat([mask, noise], 1)\n",
    "    output_warp = noise_embedding_encoder(input_warp)\n",
    "    encoder_noise = mix_weights * output_warp * mask + (1 - mix_weights) *noise * mask + noise * (1-mask) \n",
    "\n",
    "\n",
    "ksize = 7\n",
    "R_attn_mask = ptp_utils.tensor_dilate(R_attn_mask.view(1,1,resolution//8,resolution//8), ksize = ksize)[0,0].float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "images = null_inversion.noise_inference(noise, prompt.replace('<R>', 'and'))\n",
    "img0 = images[0].copy()\n",
    "\n",
    "min_height, min_width, max_height, max_width = bboxes[0]\n",
    "mix_imgs, encoder_imgs = [], []\n",
    "src_mix_imgs, src_encoder_imgs = [], []\n",
    "\n",
    "    \n",
    "images = null_inversion.noise_inference(mix_noise, prompt)\n",
    "img1 = images[0].copy()\n",
    "src_mix_imgs = images[0].copy()\n",
    "cv2.rectangle(img1, (min_width+width_fix, min_height+height_fix ), (max_width+width_fix, max_height+height_fix), (255, 0, 0), 2)\n",
    "mix_img_ = img1\n",
    "\n",
    "\n",
    "images = null_inversion.noise_inference(encoder_noise, prompt)\n",
    "img2 = images[0].copy()\n",
    "src_encoder_imgs = images[0].copy()\n",
    "cv2.rectangle(img2, (min_width+width_fix, min_height+height_fix ), (max_width+width_fix, max_height+height_fix), (255, 0, 0), 2)\n",
    "encoder_img_ = img2\n",
    "\n",
    "final_bbox = [min_height+height_fix, min_width+width_fix, max_height+height_fix, max_width+width_fix]\n",
    "# cv2.rectangle(img0, (min_width-width_fix, min_height-height_fix ), (max_width-width_fix, max_height-height_fix ), (0, 0, 255), 2)\n",
    "    \n",
    "    \n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(img0)\n",
    "axes[0].axis('off')  # 关闭坐标轴\n",
    "axes[0].set_title(\"base noise\")\n",
    "\n",
    "# 在第二列显示第二张图像\n",
    "axes[1].imshow(mix_img_)\n",
    "axes[1].axis('off')  # 关闭坐标轴\n",
    "axes[1].set_title(\"Copy-Paste\")\n",
    "# 在第二列显示第二张图像\n",
    "axes[2].imshow(encoder_img_)\n",
    "axes[2].axis('off')  # 关闭坐标轴\n",
    "axes[2].set_title(\"Ours\")\n",
    "\n",
    "# 显示图像\n",
    "plt.tight_layout()  # 自动调整子图间距\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idi = 0\n",
    "\n",
    "image_folder = \"./2-results/shake_hands\"\n",
    "os.makedirs(image_folder, exist_ok = True)\n",
    "\n",
    "name = os.path.join(image_folder, f'{idi}.pth')\n",
    "if os.path.exists(name):\n",
    "    print('ERROR: existing name of random noise!!!')\n",
    "else:\n",
    "    torch.save(noise, name )\n",
    "\n",
    "    cv2.imwrite(os.path.join(image_folder, f'{idi}-src.png'), cv2.cvtColor(img0, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(image_folder, f'{idi}-bbox-copypaste.png'), cv2.cvtColor(mix_img_, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(image_folder, f'{idi}-bbox-encodernoise.png'), cv2.cvtColor(encoder_img_, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    cv2.imwrite(os.path.join(image_folder, f'{idi}-copypaste.png'), cv2.cvtColor(src_mix_imgs, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imwrite(os.path.join(image_folder, f'{idi}-encodernoise.png'), cv2.cvtColor(src_encoder_imgs, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    paras = {}\n",
    "    paras['prompt'] = prompt\n",
    "    paras['bbox'] = final_bbox\n",
    "    paras['lora_scale'] = args.lora_scale\n",
    "    paras['exp_id'] = args.exp_id\n",
    "    paras['encoder'] = args.encoder\n",
    "\n",
    "    para_name = f'{idi}.json'\n",
    "    with open(os.path.join(image_folder,para_name), 'w') as f:\n",
    "        json.dump(paras, f, indent=2, sort_keys=True, ensure_ascii=False) \n",
    "    idi += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styleshot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
